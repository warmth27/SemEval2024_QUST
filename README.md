For the monolingual task, the submission system comprised ten models across five distinct methods, as delineated in Table \ref{model description}. The stacking ensemble method constituted the ultimate submission system, composed of the two best-performing models from among the ten.

We first compare various baseline models, including deep learning methods such as CNN, RNN, RCNN, and Self-Attention, as well as fine-tuned models, including DeBERTa\_v3\_base, DeBERTa\_v3\_large, longformer\_base\_4090, BERT, and the adapter-based DeBERTa\_v3\_large. The fine-tuned models were all trained on the translated training set and the official validation set. These models had a learning rate of 1e-4, were trained for 3 epochs, and the best-performing models on the validation set were saved. For the fine-tuned models, we adhered to the inherent 512-token length limitation to ensure consistency in the input data and effective processing by the models.
