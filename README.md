For the monolingual task, the submission system comprised ten models across five distinct methods, as delineated in Table \ref{model description}. The stacking ensemble method constituted the ultimate submission system, composed of the two best-performing models from among the ten.

We first compare various baseline models, including deep learning methods such as CNN, RNN, RCNN, and Self-Attention, as well as fine-tuned models, including DeBERTa\_v3\_base, DeBERTa\_v3\_large, longformer\_base\_4090, BERT, and the adapter-based DeBERTa\_v3\_large. The fine-tuned models were all trained on the translated training set and the official validation set. These models had a learning rate of 1e-4, were trained for 3 epochs, and the best-performing models on the validation set were saved. For the fine-tuned models, we adhered to the inherent 512-token length limitation to ensure consistency in the input data and effective processing by the models.

The final monolingual model selected the top-performing two models, DeBERTa\_v3\_large and RoBERTa\_base model based on MPU framework, for stacking ensemble learning. The learning rate for the ensemble model remained set at 1e-4, trained for 1000 epochs. Only the best-performing stacking model was retained, and the final predictions were based on this optimal stacking model.

In the multilingual track, we conducted experiments employing the top five models that exhibited promising performance in monolingual contexts. Acknowledging the dataset's multilingual nature, we opted to substitute the RoBERTa model with the XLM\_R model, which is specifically designed for multilingual tasks. Subsequently, we identified the top two models for integration through stacking ensemble. The integration of predictions from these two models yielded superior predictive performance.

In the selection of multilingual models, we performed an in-depth analysis based on the top five models selected from monolingual performance. Considering the inherent differences between multilingual and monolingual tasks, we replaced the RoBERTa component in the models with the XLM\_R model and trained them on a cleaned multilingual training dataset. We set the learning rate to 1e-4 and fixed the training epochs to 3 while retaining the models with the best performance on the validation set.

In Subtask A, considering that the RoBERTa\_base model combined with the MPU method demonstrated good performance in subtask A, we initially considered extending this to multi-class problems. We adopted the one-vs-rest strategy, transforming the binary classification into a multi-class issue. In Subtask B, there are six categories, including human, ChatGPT, etc. We trained each category separately, designating one category as the positive class and the others as negative in each iteration, thereby creating six classifiers. 

For prediction, if only one classifier identifies a category as positive, that category is deemed the classification result. If multiple classifiers predict the same category as positive, we select the category with the highest confidence level from these classifiers as the final prediction. Subsequently, we chose the consistently high-performing DeBERTa\_v3\_large-adapter model and applied it to Subtask B. Moreover, we introduced a new RoBERTa model based on adapters. Ultimately, our submitted system utilized the two best-performing models out of the three for stacked predictions. 
